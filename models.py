# -*- coding: utf-8 -*-
"""Adv_models.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/17MBKY9OukJjipQ6ttMwAtHCZnZBWFhPU
"""

import math
from collections import OrderedDict

import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F

from utils import ResBlock, DenseBlock, UpSample2x, crop_op, crop_to_shape, PaddingLayer

class ConvAE(nn.Module):

  def __init__(self,  in_ch, out_ch):

    super(ConvAE, self).__init__()

    self.in_chans = in_ch
    self.out_chans = out_ch

    module_list = [                               
              ('conv00', nn.Conv2d(3, 64, 5,  stride=1, padding=0, bias=False)),
              ('bn00', nn.BatchNorm2d(64, eps=1e-5)),
              ('relu00', nn.ReLU(inplace=True))]
    
    self.conv0 = nn.Sequential(OrderedDict(module_list))

    self.e0 = ResBlock(64, [64, 64, 256], 3, stride=1)
    self.e1 = ResBlock(256, [128, 128, 512], 4, stride=2)
    self.e2 = ResBlock(512, [256, 256, 1024], 3, stride=2)

    self.bottleneck = nn.Conv2d(1024, 512, 1, stride=1, padding=0, bias=False)

    module_list = [ 
                ("convi", nn.Conv2d(512, 128, 5, stride=1, padding=0, bias=False)),
                ("dense", DenseBlock(128, [1, 5], [128, 32], 4, split=4)),
                ("convf", nn.Conv2d(256, 256, 1, stride=1, padding=0, bias=False),),
            ]
    self.d1 = nn.Sequential(OrderedDict(module_list))

    module_list = [ 
                ("convi", nn.Conv2d(256, 64, 5, stride=1, padding=0, bias=False)),
                ("bn", nn.BatchNorm2d(64, eps=1e-5)),
                ("relu", nn.ReLU(inplace=True)),
                ("convj", nn.Conv2d(64, out_ch, 1, stride=1, padding=0, bias=True),),
            ]

    self.d0 = nn.Sequential(OrderedDict(module_list))

    self.upsample2x = UpSample2x()
    self.padding = PaddingLayer(3, 2)

  def forward(self, x):
    
    en_out0 = self.conv0(x)
    en_out0 = self.e0(en_out0)

    en_out1 = self.e1(en_out0)

    en_out2 = self.e2(en_out1)
    en_out2 = self.bottleneck(en_out2)

    en = [en_out0, en_out1, en_out2]

    ## some shapping is needed here!
    en[0] = crop_op(en[0], [40, 40])

    de_out1 = self.upsample2x(en[-1]) + en[-2]
    de_out1 = self.d1(de_out1)

    de_out0 = self.upsample2x(de_out1) + en[-3]
    de_out0 = self.d0(de_out0)

    return de_out0

class DiscriminateModel(nn.Module):

  def __init__(self,  in_ch, out_ch):

    super(DiscriminateModel, self).__init__()

    self.in_chans = in_ch
    self.out_chans = out_ch

    module_list = [                              
          ('convD1', nn.Conv2d(in_ch, 64, 5, stride=2, padding=0, bias=False)),
          ('reluD1', nn.ReLU(inplace=True)),            
          ('convD2', nn.Conv2d(64, 128, 5, stride=2, padding=0, bias=False)),
          ('reluD2', nn.ReLU(inplace=True)),
          ('convD3', nn.Conv2d(128, 256, 5, stride=2, padding=0, bias=False)),
          ('reluD3', nn.ReLU(inplace=True)),
          ('convD4', nn.Conv2d(256, out_ch, 3, stride=2, padding=0, bias=True))
          ]

    self.Dc = nn.Sequential(OrderedDict(module_list))

  def forward(self, x):
    out = torch.flatten(self.Dc(x))
    return out

